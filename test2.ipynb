{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理与增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 配置路径\n",
    "data_root = \"E:/CPCI/plantdoc/\"\n",
    "image_dir = \"E:/CPCI/plantdoc/images/\"\n",
    "split_file = data_root + \"trainval.txt\"\n",
    "prompt_file = data_root + \"plantwild_prompts.json\"\n",
    "\n",
    "# 数据增强\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 加载数据划分\n",
    "def load_split(split_path):\n",
    "    splits = {'train': [], 'val': [], 'test': []}\n",
    "    with open(split_path) as f:\n",
    "        for line in f:\n",
    "            path, class_id, split_id = line.strip().split('=')\n",
    "            split_type = ['test', 'train', 'val'][int(split_id)]\n",
    "            splits[split_type].append((path, int(class_id)))\n",
    "    return splits\n",
    "\n",
    "# 加载文本提示\n",
    "with open(prompt_file) as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "# 自定义数据集\n",
    "class PlantDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split_data, transform=None):\n",
    "        self.samples = split_data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, class_id = self.samples[idx]\n",
    "        img_path = os.path.join(image_dir, path)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text_prompts = prompts[str(class_id)]  # 获取50个文本提示\n",
    "        return image, class_id, text_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多模态模型构建（使用CLIP + 微调）\n",
    "模型下载链接：https://github.com/openai/CLIP （需安装clip包）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # 加载本地预训练CLIP模型\n",
    "        model_path = \"E:\\models\\CLIP_VIT-L-14\\pytorch_model.bin\"\n",
    "        config_path = \"E:\\models\\CLIP_VIT-L-14\\config.json\"\n",
    "        tokenizer_path = \"E:\\models\\CLIP_VIT-L-14\\\\tokenizer.json\"\n",
    "        \n",
    "        # 加载分词器\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        # 加载模型配置\n",
    "        self.clip_model, _ = clip.load(config_path, device='cpu', jit=False)\n",
    "        \n",
    "        # 加载模型权重\n",
    "        state_dict = torch.load(model_path, map_location='cpu')\n",
    "        self.clip_model.load_state_dict(state_dict)\n",
    "        \n",
    "        # 冻结部分参数\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 替换最后的分类层\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Linear(1024, 256),  # 根据模型输出维度调整\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(1024, 256),  # 根据模型输出维度调整\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.final_classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # 图像特征提取\n",
    "        image_features = self.clip_model.encode_image(images)\n",
    "        image_features = self.image_fc(image_features)\n",
    "        \n",
    "        # 文本特征提取（取平均）\n",
    "        text_tokens = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(images.device)\n",
    "        text_features = self.clip_model.encode_text(text_tokens.input_ids)\n",
    "        text_features = self.text_fc(text_features)\n",
    "        \n",
    "        # 特征融合\n",
    "        combined = torch.cat([image_features, text_features], dim=1)\n",
    "        return self.final_classifier(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练策略优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SYH\\AppData\\Local\\Temp\\ipykernel_28460\\3586251259.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location='cpu')\n",
      "d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# 初始化\n",
    "splits = load_split(split_file)\n",
    "train_dataset = PlantDataset(splits['train'], image_transform)\n",
    "val_dataset = PlantDataset(splits['val'], image_transform)\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # 加载本地预训练CLIP模型\n",
    "        model_path = \"E:\\models\\CLIP_VIT-L-14\\pytorch_model.bin\"\n",
    "        config_path = \"E:\\models\\CLIP_VIT-L-14\\config.json\"\n",
    "        tokenizer_path = \"E:\\models\\CLIP_VIT-L-14\"\n",
    "        \n",
    "        # 加载分词器\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        # 加载模型配置\n",
    "        self.clip_model = CLIPModel.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        # 加载模型权重\n",
    "        state_dict = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # 移除不匹配的键\n",
    "        keys_to_remove = ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n",
    "        for key in keys_to_remove:\n",
    "            if key in state_dict:\n",
    "                del state_dict[key]\n",
    "        \n",
    "        self.clip_model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        # 冻结部分参数\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 替换最后的分类层\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # 根据模型输出维度调整\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # 根据模型输出维度调整\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.final_classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # 图像特征提取\n",
    "        image_features = self.clip_model.vision_model(images).last_hidden_state[:, 0, :]\n",
    "        image_features = self.image_fc(image_features)\n",
    "        \n",
    "        # 文本特征提取（取平均）\n",
    "        text_tokens = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(images.device)\n",
    "        text_features = self.clip_model.text_model(text_tokens.input_ids).last_hidden_state[:, 0, :]\n",
    "        text_features = self.text_fc(text_features)\n",
    "        \n",
    "        # 特征融合\n",
    "        combined = torch.cat([image_features, text_features], dim=1)\n",
    "        return self.final_classifier(combined)\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "model = MultimodalClassifier(num_classes=89)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 自定义collate_fn处理文本\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.tensor([item[1] for item in batch])\n",
    "    texts = [item[2][torch.randint(0, 50, (1,))[0]] for item in batch]  # 随机选一个提示\n",
    "    return images, labels, texts\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# 训练循环\n",
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels, texts in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 验证循环\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, texts in loader:\n",
    "            outputs = model(images, texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/split_file.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# 初始化\u001b[39;00m\n\u001b[0;32m     77\u001b[0m split_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/split_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# 替换为实际的 split 文件路径\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43mload_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PlantDataset(splits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], image_transform)\n\u001b[0;32m     80\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m PlantDataset(splits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m], image_transform)\n",
      "Cell \u001b[1;32mIn[23], line 56\u001b[0m, in \u001b[0;36mload_split\u001b[1;34m(split_file)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_split\u001b[39m(split_file):\n\u001b[0;32m     55\u001b[0m     splits \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m     58\u001b[0m             image_path, class_id, split \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/split_file.csv'"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 load_split 和 PlantDataset 已经定义\n",
    "# splits = load_split(split_file)\n",
    "# train_dataset = PlantDataset(splits['train'], image_transform)\n",
    "# val_dataset = PlantDataset(splits['val'], image_transform)\n",
    "\n",
    "class PlantDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.prompts = {\n",
    "            '0': [\"prompt1\", \"prompt2\", \"prompt3\"],  # 示例提示\n",
    "            '1': [\"promptA\", \"promptB\", \"promptC\"],  # 示例提示\n",
    "            # 添加其他类的提示\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, class_id, prompts = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 打印调试信息\n",
    "        print(f\"Class ID: {class_id}, Prompts: {self.prompts.keys()}\")\n",
    "        \n",
    "        try:\n",
    "            text_prompts = self.prompts[str(class_id)]  # 获取50个文本提示\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: Class ID {class_id} not found in prompts dictionary.\")\n",
    "            raise\n",
    "        \n",
    "        return image, class_id, text_prompts\n",
    "\n",
    "# 初始化\n",
    "splits = load_split(split_file)\n",
    "train_dataset = PlantDataset(splits['train'], image_transform)\n",
    "val_dataset = PlantDataset(splits['val'], image_transform)\n",
    "\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # 加载本地预训练CLIP模型\n",
    "        model_path = \"E:\\models\\CLIP_VIT-L-14\\pytorch_model.bin\"\n",
    "        config_path = \"E:\\models\\CLIP_VIT-L-14\\config.json\"\n",
    "        tokenizer_path = \"E:\\models\\CLIP_VIT-L-14\"\n",
    "        \n",
    "        # 加载分词器\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        # 加载模型配置\n",
    "        self.clip_model = CLIPModel.from_pretrained(tokenizer_path)\n",
    "        \n",
    "        # 加载模型权重\n",
    "        state_dict = torch.load(model_path, map_location='cpu')\n",
    "        \n",
    "        # 移除不匹配的键\n",
    "        keys_to_remove = ['text_model.embeddings.position_ids', 'vision_model.embeddings.position_ids']\n",
    "        for key in keys_to_remove:\n",
    "            if key in state_dict:\n",
    "                del state_dict[key]\n",
    "        \n",
    "        self.clip_model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        # 冻结部分参数\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # 替换最后的分类层\n",
    "        self.image_fc = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # 根据模型输出维度调整\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(768, 256),  # 根据模型输出维度调整\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.final_classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # 图像特征提取\n",
    "        image_features = self.clip_model.vision_model(images).last_hidden_state[:, 0, :]\n",
    "        image_features = self.image_fc(image_features)\n",
    "        \n",
    "        # 文本特征提取（取平均）\n",
    "        text_tokens = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(images.device)\n",
    "        text_features = self.clip_model.text_model(text_tokens.input_ids).last_hidden_state[:, 0, :]\n",
    "        text_features = self.text_fc(text_features)\n",
    "        \n",
    "        # 特征融合\n",
    "        combined = torch.cat([image_features, text_features], dim=1)\n",
    "        return self.final_classifier(combined)\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "model = MultimodalClassifier(num_classes=89)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 自定义collate_fn处理文本\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.tensor([item[1] for item in batch])\n",
    "    texts = [item[2][torch.randint(0, 50, (1,))[0]] for item in batch]  # 随机选一个提示\n",
    "    return images, labels, texts\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# 训练循环\n",
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels, texts in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# 验证循环\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, texts in loader:\n",
    "            outputs = model(images, texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# 训练和验证\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    val_accuracy = evaluate(model, val_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# 绘制训练损失和验证准确率曲线\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_accuracies, label='Val Accuracy', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创新改进点（需集成到代码中）：\n",
    "文本提示增强：对每个样本随机选择3个提示进行特征融合\n",
    "注意力融合模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.image_proj = nn.Linear(dim, dim)\n",
    "        self.text_proj = nn.Linear(dim, dim)\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads=4)\n",
    "        \n",
    "    def forward(self, image_feat, text_feat):\n",
    "        image_proj = self.image_proj(image_feat).unsqueeze(1)\n",
    "        text_proj = self.text_proj(text_feat).unsqueeze(1)\n",
    "        combined = torch.cat([image_proj, text_proj], dim=1)\n",
    "        attn_output, _ = self.attention(combined, combined, combined)\n",
    "        return attn_output.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型保存与推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(image_path, model):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image_transform(image).unsqueeze(0)\n",
    "    class_id = 0  # 根据实际类别修改\n",
    "    texts = prompts[str(class_id)][:5]  # 取前5个提示\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image, texts)\n",
    "    return torch.softmax(output, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvpdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
