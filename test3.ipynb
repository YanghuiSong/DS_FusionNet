{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'PlantDataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# 类似训练流程，计算准确率等指标\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     evaluate()\n",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m model \u001b[38;5;241m=\u001b[39m EnhancedCLIP()\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PlantDataset(split_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     97\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\utils\\data\\sampler.py:150\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'PlantDataset' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 配置参数\n",
    "config = {\n",
    "    \"data_path\": \"E:/CPCI/plantdoc\",\n",
    "    \"model_path\": \"E:/models/CLIP_VIT-L-14\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "    \"num_prompts\": 50,\n",
    "    \"image_size\": 224,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "# 数据加载器\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, split_type='train'):\n",
    "        self.split_map = {'train':1, 'val':2, 'test':0}\n",
    "        with open(f\"{config['data_path']}/trainval.txt\") as f:\n",
    "            self.items = [line.strip().split('=') for line in f]\n",
    "        \n",
    "        with open(f\"{config['data_path']}/plantwild_prompts.json\") as f:\n",
    "            self.prompts = json.load(f)\n",
    "            \n",
    "        self.processor = CLIPProcessor.from_pretrained(config['model_path'])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_id, split_id = self.items[idx]\n",
    "        img = Image.open(f\"{config['data_path']}/images/{img_path}\")\n",
    "        \n",
    "        # 文本处理\n",
    "        class_name = self._get_class_name(class_id)\n",
    "        text_inputs = self.processor(\n",
    "            text=self.prompts[class_name][:config['num_prompts']],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 图像处理\n",
    "        image_inputs = self.processor(\n",
    "            images=img,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image_inputs.pixel_values.squeeze(),\n",
    "            'text': text_inputs.input_ids,\n",
    "            'attention_mask': text_inputs.attention_mask,\n",
    "            'label': int(class_id)\n",
    "        }\n",
    "\n",
    "# 改进模型\n",
    "class EnhancedCLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(config['model_path'])\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "        self.fc = nn.Linear(768*2, 89)  # 89 classes\n",
    "        \n",
    "    def forward(self, images, texts):\n",
    "        # 图像特征\n",
    "        image_features = self.clip.vision_model(images).last_hidden_state[:,0,:]\n",
    "        \n",
    "        # 文本特征\n",
    "        text_features = self.clip.text_model(\n",
    "            input_ids=texts.input_ids,\n",
    "            attention_mask=texts.attention_mask\n",
    "        ).last_hidden_state[:,0,:]\n",
    "        \n",
    "        # 跨模态注意力\n",
    "        attn_out, _ = self.cross_attn(\n",
    "            image_features.unsqueeze(1),\n",
    "            text_features.unsqueeze(1),\n",
    "            text_features.unsqueeze(1)\n",
    "        )\n",
    "        \n",
    "        # 特征融合\n",
    "        fused_features = torch.cat([\n",
    "            image_features,\n",
    "            attn_out.squeeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        return self.fc(fused_features)\n",
    "\n",
    "# 训练流程\n",
    "def train():\n",
    "    model = EnhancedCLIP().to(config['device'])\n",
    "    dataset = PlantDataset(split_type='train')\n",
    "    dataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(config['device'])\n",
    "            texts = batch['text'].to(config['device'])\n",
    "            labels = batch['label'].to(config['device'])\n",
    "            \n",
    "            outputs = model(images, texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch} Loss: {loss.item()}\")\n",
    "\n",
    "# 评估代码\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    # 类似训练流程，计算准确率等指标\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、数据加载器优化（适配实际文件结构）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, split_type='train'):\n",
    "        \"\"\"\n",
    "        split_type: 'train'(1), 'val'(2), 'test'(0)\n",
    "        \"\"\"\n",
    "        self.split_code = {'train':1, 'val':2, 'test':0}[split_type]\n",
    "        self.data_root = \"E:/CPCI/plantdoc\"\n",
    "        \n",
    "        # 加载类别映射\n",
    "        self.class_map = {}\n",
    "        with open(os.path.join(self.data_root, 'classes.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                class_id, class_name = line.strip().split(' ', 1)\n",
    "                self.class_map[int(class_id)] = class_name\n",
    "        \n",
    "        # 加载划分数据\n",
    "        self.samples = []\n",
    "        with open(os.path.join(self.data_root, 'trainval.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('=')\n",
    "                if len(parts) < 3: continue\n",
    "                img_relpath, class_id, split_id = parts[0], int(parts[1]), int(parts[2])\n",
    "                if split_id == self.split_code:\n",
    "                    self.samples.append((\n",
    "                        os.path.join(self.data_root, 'images', img_relpath),\n",
    "                        class_id\n",
    "                    ))\n",
    "        \n",
    "        # 加载文本提示\n",
    "        with open(os.path.join(self.data_root, 'plantwild_prompts.json'), 'r') as f:\n",
    "            self.prompts = json.load(f)\n",
    "        \n",
    "        # CLIP预处理\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"E:/models/CLIP_VIT-L-14\")\n",
    "        \n",
    "        # 数据增强\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                               (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "        \n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                               (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_id = self.samples[idx]\n",
    "        \n",
    "        # 加载图像\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        transform = self.train_transform if self.split_code == 1 else self.test_transform\n",
    "        image = transform(img)\n",
    "        \n",
    "        # 获取类别文本提示\n",
    "        class_name = self.class_map[class_id]\n",
    "        text_prompts = self.prompts.get(class_name, [class_name])[:50]  # 取前50个提示\n",
    "        \n",
    "        # 随机选择一个提示进行训练\n",
    "        selected_prompt = text_prompts[torch.randint(0, len(text_prompts), (1,)).item()]\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': selected_prompt,\n",
    "            'label': class_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持多模态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "class EnhancedCLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 加载CLIP基础模型\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"E:/models/CLIP_VIT-L-14\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"E:/models/CLIP_VIT-L-14\")\n",
    "        self.tokenizer = self.clip_processor.tokenizer\n",
    "        \n",
    "        # 多模态融合层\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768*2, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512)\n",
    "        )\n",
    "        \n",
    "        # 分类头\n",
    "        self.classifier = nn.Linear(512, 89)\n",
    "        \n",
    "        # 可学习的提示参数\n",
    "        self.prompt_weights = nn.Parameter(torch.ones(50))\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # 图像特征\n",
    "        image_features = self.clip_model.get_image_features(pixel_values=images)\n",
    "        \n",
    "        # 文本特征\n",
    "        text_inputs = self.tokenizer(\n",
    "            texts, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(images.device)\n",
    "        text_features = self.clip_model.get_text_features(**text_inputs)\n",
    "        \n",
    "        # 特征融合\n",
    "        fused = torch.cat([image_features, text_features], dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        \n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三、完整训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m,  \u001b[38;5;66;03m# 减少批量大小\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccumulation_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,  \u001b[38;5;66;03m# 梯度累积步骤\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m }\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 初始化\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEnhancedCLIP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m train_set \u001b[38;5;241m=\u001b[39m PlantDataset(split_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     60\u001b[0m val_set \u001b[38;5;241m=\u001b[39m PlantDataset(split_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 780 (1 times)]\u001b[0m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "class EnhancedCLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 加载CLIP基础模型\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"E:/models/CLIP_VIT-L-14\")\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"E:/models/CLIP_VIT-L-14\")\n",
    "        self.tokenizer = self.clip_processor.tokenizer\n",
    "        \n",
    "        # 多模态融合层\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(768*2, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LayerNorm(512)\n",
    "        )\n",
    "        \n",
    "        # 分类头\n",
    "        self.classifier = nn.Linear(512, 89)\n",
    "        \n",
    "        # 可学习的提示参数\n",
    "        self.prompt_weights = nn.Parameter(torch.ones(50))\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        # 图像特征\n",
    "        image_features = self.clip_model.get_image_features(pixel_values=images)\n",
    "        \n",
    "        # 文本特征\n",
    "        text_inputs = self.tokenizer(\n",
    "            texts, \n",
    "            padding=True, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(images.device)\n",
    "        text_features = self.clip_model.get_text_features(**text_inputs)\n",
    "        \n",
    "        # 特征融合\n",
    "        fused = torch.cat([image_features, text_features], dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        \n",
    "        return self.classifier(fused)\n",
    "\n",
    "def train():\n",
    "    # 配置参数\n",
    "    config = {\n",
    "        \"batch_size\": 16,  # 减少批量大小\n",
    "        \"accumulation_steps\": 2,  # 梯度累积步骤\n",
    "        \"lr\": 1e-5,\n",
    "        \"epochs\": 20,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "    \n",
    "    # 初始化\n",
    "    model = EnhancedCLIP().to(config['device'])\n",
    "    train_set = PlantDataset(split_type='train')\n",
    "    val_set = PlantDataset(split_type='val')\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=config['batch_size'], num_workers=4, pin_memory=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.clip_model.parameters(), 'lr': 1e-6},\n",
    "        {'params': model.fusion.parameters()},\n",
    "        {'params': model.classifier.parameters()}\n",
    "    ], lr=config['lr'])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 训练循环\n",
    "    best_acc = 0\n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            images = batch['image'].to(config['device'])\n",
    "            texts = batch['text']\n",
    "            labels = batch['label'].to(config['device'])\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(images, texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss = loss / config['accumulation_steps']  # 梯度累积\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % config['accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * config['accumulation_steps']\n",
    "        \n",
    "        # 验证\n",
    "        val_acc = evaluate(model, val_loader, config['device'])\n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} | Loss: {total_loss/len(train_loader):.4f} | Val Acc: {val_acc:.2%}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_acc > best_acc:\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            best_acc = val_acc\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch['image'].to(device)\n",
    "            texts = batch['text']\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images, texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvpdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
