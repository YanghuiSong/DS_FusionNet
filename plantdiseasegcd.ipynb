{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 改进的数据集类（支持半监督设置）\n",
    "class SemiSupervisedPlantDoc(Dataset):\n",
    "    def __init__(self, root_dir, labeled_ratio=0.1, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.labeled_samples = []\n",
    "        self.unlabeled_samples = []\n",
    "        \n",
    "        # 获取所有类别\n",
    "        class_to_idx = {}\n",
    "        for idx, cls in enumerate(os.listdir(os.path.join(root_dir, 'train'))):\n",
    "            class_to_idx[cls] = idx\n",
    "        \n",
    "        # 解析标记数据\n",
    "        for cls in class_to_idx:\n",
    "            class_dir = os.path.join(root_dir, 'train', cls)\n",
    "            images = os.listdir(class_dir)\n",
    "            split_idx = int(len(images) * labeled_ratio)\n",
    "            \n",
    "            for i, img_name in enumerate(images):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                label = class_to_idx[cls]\n",
    "                \n",
    "                if i < split_idx:  # 标记数据\n",
    "                    self.labeled_samples.append((img_path, label))\n",
    "                else:  # 未标记数据\n",
    "                    self.unlabeled_samples.append((img_path, -1))  # -1表示未标记\n",
    "                    \n",
    "        # 合并标记和未标记数据\n",
    "        self.samples = self.labeled_samples + self.unlabeled_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        is_labeled = (label != -1)\n",
    "        return image, label, is_labeled\n",
    "\n",
    "# 数据预处理配置\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 初始化数据集\n",
    "data_root = 'E:\\data1\\PlantDisease'  # 根据实际路径修改\n",
    "full_dataset = SemiSupervisedPlantDoc(\n",
    "    root_dir=data_root,\n",
    "    labeled_ratio=0.1,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# 数据加载器（混合标记和未标记数据）\n",
    "train_loader = DataLoader(full_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# 加载DINOv2模型\n",
    "class DINOv2Wrapper(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        return self.projection(features)\n",
    "\n",
    "model = DINOv2Wrapper(r'E:\\models\\facebook_Dinov2')\n",
    "\n",
    "# 对比损失函数\n",
    "class GCDLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def supervised_contrastive(self, features, labels):\n",
    "        labels = labels.unsqueeze(1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        \n",
    "        logits = torch.matmul(features, features.T) / self.temperature\n",
    "        logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "        logits = logits - logits_max.detach()\n",
    "        \n",
    "        exp_logits = torch.exp(logits)\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "        \n",
    "        loss = - (mask * log_prob).sum(1) / mask.sum(1)\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, features, labels, is_labeled):\n",
    "        # 标记数据的监督对比损失\n",
    "        labeled_mask = torch.where(is_labeled)[0]\n",
    "        if len(labeled_mask) > 1:\n",
    "            sup_loss = self.supervised_contrastive(\n",
    "                features[labeled_mask], \n",
    "                labels[labeled_mask]\n",
    "            )\n",
    "        else:\n",
    "            sup_loss = torch.tensor(0.0)\n",
    "            \n",
    "        # 未标记数据的噪声对比估计\n",
    "        logits = torch.matmul(features, features.T) / self.temperature\n",
    "        labels = torch.arange(logits.size(0)).to(logits.device)\n",
    "        \n",
    "        nce_loss = (self.cross_entropy(logits, labels) + \n",
    "                   self.cross_entropy(logits.T, labels)) / 2\n",
    "        \n",
    "        return 0.5*sup_loss + 0.5*nce_loss\n",
    "\n",
    "# 半监督聚类模块\n",
    "class SemiSupervisedKMeans:\n",
    "    def __init__(self, n_known_classes):\n",
    "        self.n_known = n_known_classes\n",
    "        self.kmeans = KMeans()\n",
    "        \n",
    "    def fit(self, features, labels=None):\n",
    "        # 使用标记数据初始化已知类中心\n",
    "        known_features = features[labels != -1]\n",
    "        known_labels = labels[labels != -1]\n",
    "        \n",
    "        # 计算已知类中心\n",
    "        known_centers = []\n",
    "        for c in range(self.n_known):\n",
    "            mask = (known_labels == c)\n",
    "            if mask.sum() > 0:\n",
    "                known_centers.append(known_features[mask].mean(axis=0))\n",
    "                \n",
    "        # 估计未知类数量\n",
    "        self.estimate_cluster_num(features)\n",
    "        \n",
    "        # 执行半监督K-means\n",
    "        self.kmeans = KMeans(n_clusters=self.total_clusters, init=np.vstack([\n",
    "            np.array(known_centers),\n",
    "            self.initialize_unknown_centers(features)\n",
    "        ]))\n",
    "        \n",
    "        self.kmeans.fit(features)\n",
    "        \n",
    "    def estimate_cluster_num(self, features):\n",
    "        # 论文中的自适应估计方法\n",
    "        candidate_n = [self.n_known + i for i in range(1, 6)]\n",
    "        best_score = -np.inf\n",
    "        best_n = self.n_known + 1\n",
    "        \n",
    "        for n in candidate_n:\n",
    "            kmeans = KMeans(n_clusters=n).fit(features)\n",
    "            score = silhouette_score(features, kmeans.labels_)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_n = n\n",
    "                \n",
    "        self.total_clusters = best_n\n",
    "        \n",
    "    def initialize_unknown_centers(self, features):\n",
    "        # 基于特征空间的密度初始化\n",
    "        distances = np.linalg.norm(features - features.mean(axis=0), axis=1)\n",
    "        return features[np.argsort(distances)[-self.total_clusters:]]\n",
    "\n",
    "# 训练流程\n",
    "def train_gcd(model, dataloader, epochs=50):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    loss_fn = GCDLoss()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            images, labels, is_labeled = batch\n",
    "            images, labels, is_labeled = images.to(device), labels.to(device), is_labeled.to(device)\n",
    "            features = model(images)\n",
    "            \n",
    "            loss = loss_fn(features, labels, is_labeled)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # 每个epoch后执行聚类和评估\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        evaluate_clustering(model, dataloader, device)\n",
    "        \n",
    "def evaluate_clustering(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = model(images)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "    features = np.concatenate(all_features)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # 执行半监督聚类\n",
    "    cluster_model = SemiSupervisedKMeans(n_known_classes=num_known_classes)\n",
    "    cluster_model.fit(features, labels)\n",
    "    \n",
    "    # 计算聚类指标\n",
    "    known_mask = (labels != -1)\n",
    "    ari = adjusted_rand_score(labels[known_mask], cluster_model.kmeans.labels_[known_mask])\n",
    "    print(f\"Adjusted Rand Index (Known Classes): {ari:.4f}\")\n",
    "\n",
    "# 获取类别数量\n",
    "num_known_classes = len(datasets.ImageFolder(os.path.join(data_root, 'train')).classes)\n",
    "print(f\"Number of known classes: {num_known_classes}\")\n",
    "\n",
    "# 开始训练\n",
    "train_gcd(model, train_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# 改进的数据集类（支持半监督设置）\n",
    "class SemiSupervisedPlantDoc(Dataset):\n",
    "    def __init__(self, root_dir, labeled_ratio=0.1, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.labeled_samples = []\n",
    "        self.unlabeled_samples = []\n",
    "        \n",
    "        # 获取所有类别\n",
    "        class_to_idx = {}\n",
    "        for idx, cls in enumerate(sorted(os.listdir(os.path.join(root_dir, 'train')))):\n",
    "            class_to_idx[cls] = idx\n",
    "        \n",
    "        # 解析标记数据\n",
    "        for cls in class_to_idx:\n",
    "            class_dir = os.path.join(root_dir, 'train', cls)\n",
    "            images = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            split_idx = int(len(images) * labeled_ratio)\n",
    "            \n",
    "            for i, img_name in enumerate(images):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                label = class_to_idx[cls]\n",
    "                \n",
    "                if i < split_idx:  # 标记数据\n",
    "                    self.labeled_samples.append((img_path, label))\n",
    "                else:  # 未标记数据\n",
    "                    self.unlabeled_samples.append((img_path, -1))  # -1表示未标记\n",
    "                    \n",
    "        # 合并标记和未标记数据\n",
    "        self.samples = self.labeled_samples + self.unlabeled_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        is_labeled = (label != -1)\n",
    "        return image, label, is_labeled\n",
    "\n",
    "# 数据预处理配置（移除归一化）\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor()  # 只转换到[0,1]范围\n",
    "])\n",
    "\n",
    "# 加载DINOv2模型（修正预处理）\n",
    "class DINOv2Wrapper(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = self.processor(images=x, return_tensors=\"pt\").pixel_values.to(x.device)\n",
    "        features = self.model(pixel_values=inputs).last_hidden_state.mean(dim=1)\n",
    "        return self.projection(features)\n",
    "\n",
    "# 对比损失函数（保持不变）\n",
    "class GCDLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def supervised_contrastive(self, features, labels):\n",
    "        labels = labels.unsqueeze(1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        \n",
    "        logits = torch.matmul(features, features.T) / self.temperature\n",
    "        logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "        logits = logits - logits_max.detach()\n",
    "        \n",
    "        exp_logits = torch.exp(logits)\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "        \n",
    "        loss = - (mask * log_prob).sum(1) / mask.sum(1)\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, features, labels, is_labeled):\n",
    "        labeled_mask = torch.where(is_labeled)[0]\n",
    "        sup_loss = self.supervised_contrastive(features[labeled_mask], labels[labeled_mask]) if len(labeled_mask) > 1 else 0.0\n",
    "        \n",
    "        logits = torch.matmul(features, features.T) / self.temperature\n",
    "        labels = torch.arange(logits.size(0)).to(logits.device)\n",
    "        nce_loss = (self.cross_entropy(logits, labels) + self.cross_entropy(logits.T, labels)) / 2\n",
    "        \n",
    "        return 0.5*sup_loss + 0.5*nce_loss\n",
    "\n",
    "# 半监督聚类模块（保持不变）\n",
    "class SemiSupervisedKMeans:\n",
    "    def __init__(self, n_known_classes):\n",
    "        self.n_known = n_known_classes\n",
    "        self.kmeans = KMeans()\n",
    "        \n",
    "    def fit(self, features, labels=None):\n",
    "        known_features = features[labels != -1]\n",
    "        known_labels = labels[labels != -1]\n",
    "        known_centers = [known_features[known_labels == c].mean(axis=0) for c in range(self.n_known) if (known_labels == c).sum() > 0]\n",
    "        \n",
    "        self.estimate_cluster_num(features)\n",
    "        init_centers = np.array(known_centers)\n",
    "        \n",
    "        # 计算需要额外添加的中心数量\n",
    "        additional_centers_needed = self.total_clusters - len(init_centers)\n",
    "        if additional_centers_needed > 0:\n",
    "            sorted_indices = np.argsort(np.linalg.norm(features - features.mean(axis=0), axis=1))[::-1]\n",
    "            additional_centers = features[sorted_indices[:additional_centers_needed]]\n",
    "            init_centers = np.vstack([init_centers, additional_centers])\n",
    "        \n",
    "        self.kmeans = KMeans(n_clusters=self.total_clusters, init=init_centers)\n",
    "        self.kmeans.fit(features)\n",
    "        \n",
    "    def estimate_cluster_num(self, features):\n",
    "        candidate_n = [self.n_known + i for i in range(1, 6)]\n",
    "        self.total_clusters = max([(n, silhouette_score(features, KMeans(n).fit(features).labels_)) for n in candidate_n], \n",
    "                                 key=lambda x: x[1])[0]\n",
    "\n",
    "# 训练流程（添加设备处理）\n",
    "def train_gcd(model, dataloader, epochs=50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    loss_fn = GCDLoss().to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for images, labels, is_labeled in dataloader:\n",
    "            images, labels, is_labeled = images.to(device), labels.to(device), is_labeled.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            features = model(images)\n",
    "            loss = loss_fn(features, labels, is_labeled)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        evaluate_clustering(model, dataloader, device)\n",
    "\n",
    "def evaluate_clustering(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_features, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in dataloader:\n",
    "            features = model(images.to(device)).cpu().numpy()\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    features = np.concatenate(all_features)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    cluster_model = SemiSupervisedKMeans(num_known_classes)\n",
    "    cluster_model.fit(features, labels)\n",
    "    \n",
    "    known_mask = (labels != -1)\n",
    "    ari = adjusted_rand_score(labels[known_mask], cluster_model.kmeans.labels_[known_mask])\n",
    "    print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "\n",
    "# 初始化配置\n",
    "data_root = 'E:\\data1\\PlantDisease'  # 替换为实际数据路径\n",
    "num_known_classes = len(os.listdir(os.path.join(data_root, 'train')))\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "full_dataset = SemiSupervisedPlantDoc(\n",
    "    root_dir=data_root,\n",
    "    labeled_ratio=0.1,\n",
    "    transform=train_transform\n",
    ")\n",
    "train_loader = DataLoader(full_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# 初始化模型\n",
    "model = DINOv2Wrapper(\"E:\\models\\\\facebook_Dinov2\")  # 使用正确的模型名称\n",
    "\n",
    "# 开始训练\n",
    "train_gcd(model, train_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, normalized_mutual_info_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# 改进的数据集类\n",
    "class SemiSupervisedPlantDoc(Dataset):\n",
    "    def __init__(self, root_dir, labeled_ratio=0.1):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        class_to_idx = {cls:i for i, cls in enumerate(sorted(os.listdir(os.path.join(root_dir, 'train'))))}\n",
    "        self.samples = []\n",
    "        \n",
    "        # 构建半监督数据集\n",
    "        for cls, idx in class_to_idx.items():\n",
    "            cls_dir = os.path.join(root_dir, 'train', cls)\n",
    "            images = [os.path.join(cls_dir, f) for f in os.listdir(cls_dir) if f.endswith(('.jpg','png','jpeg'))]\n",
    "            split = int(len(images)*labeled_ratio)\n",
    "            \n",
    "            for i, img_path in enumerate(images):\n",
    "                if i < split:  # 标记样本\n",
    "                    self.samples.append((img_path, idx))\n",
    "                else:  # 未标记样本\n",
    "                    self.samples.append((img_path, -1))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(img), label, (label != -1)\n",
    "\n",
    "# 增强的DINOv2模型\n",
    "class EnhancedDINOv2(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # 冻结主干网络\n",
    "            inputs = self.processor(x, return_tensors=\"pt\").pixel_values.to(x.device)\n",
    "            features = self.backbone(pixel_values=inputs).last_hidden_state.mean(dim=1)\n",
    "        return self.projection(features)\n",
    "\n",
    "# 改进的对比损失\n",
    "class EnhancedGCDLoss(nn.Module):\n",
    "    def __init__(self, temp=0.1):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.ce = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        \n",
    "    def supervised_loss(self, f, labels):\n",
    "        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0))\n",
    "        logits = torch.matmul(f, f.T) / self.temp\n",
    "        logits = logits - torch.max(logits, dim=1, keepdim=True)[0].detach()\n",
    "        return -torch.mean(mask * torch.log_softmax(logits, dim=1))\n",
    "    \n",
    "    def forward(self, features, labels, is_labeled):\n",
    "        labeled = features[is_labeled]\n",
    "        sup_loss = self.supervised_loss(labeled, labels[is_labeled]) if labeled.size(0) > 1 else 0\n",
    "        \n",
    "        # 自监督对比\n",
    "        logits = torch.matmul(features, features.T) / self.temp\n",
    "        ssl_loss = self.ce(logits, labels)\n",
    "        \n",
    "        return 0.7*sup_loss + 0.3*ssl_loss\n",
    "\n",
    "# 改进的聚类模块\n",
    "class EnhancedClusterer:\n",
    "    def __init__(self, n_known):\n",
    "        self.n_known = n_known\n",
    "        \n",
    "    def fit_predict(self, features, labels):\n",
    "        known_features = features[labels != -1]\n",
    "        known_labels = labels[labels != -1]\n",
    "        \n",
    "        # 计算已知类别中心\n",
    "        unique_known_labels = np.unique(known_labels)\n",
    "        if len(unique_known_labels) == 0:\n",
    "            print(\"No known labels found.\")\n",
    "            return np.full_like(labels, -1)\n",
    "        \n",
    "        known_centers = {}\n",
    "        for c in unique_known_labels:\n",
    "            known_centers[c] = known_features[known_labels == c].mean(axis=0)\n",
    "        \n",
    "        # 使用DBSCAN来发现新类别\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        db_labels = dbscan.fit_predict(features)\n",
    "        \n",
    "        # 合并已知标签和DBSCAN结果\n",
    "        unique_db_labels = set(db_labels)\n",
    "        new_label_counter = max(unique_known_labels) + 1 if unique_known_labels.size > 0 else 0\n",
    "        \n",
    "        final_labels = np.full_like(db_labels, -1)\n",
    "        for lbl in unique_db_labels:\n",
    "            if lbl == -1:  # 噪声点\n",
    "                continue\n",
    "            \n",
    "            cluster_indices = np.where(db_labels == lbl)[0]\n",
    "            cluster_features = features[cluster_indices]\n",
    "            \n",
    "            if len(cluster_features) < 5:  # 过滤掉小簇\n",
    "                continue\n",
    "            \n",
    "            closest_center_idx = min(known_centers.keys(), key=lambda k: np.linalg.norm(cluster_features.mean(axis=0) - known_centers[k]), default=None)\n",
    "            if closest_center_idx is None:\n",
    "                # 如果没有找到最近的已知中心，则认为这是一个新类别\n",
    "                final_labels[cluster_indices] = new_label_counter\n",
    "                new_label_counter += 1\n",
    "            elif not self._is_close_to_any_known(cluster_features.mean(axis=0), list(known_centers.values()), threshold=0.5):\n",
    "                final_labels[cluster_indices] = new_label_counter\n",
    "                new_label_counter += 1\n",
    "            else:\n",
    "                final_labels[cluster_indices] = closest_center_idx\n",
    "        \n",
    "        # 确保聚类数量至少为已知类别数\n",
    "        if len(np.unique(final_labels)) < self.n_known:\n",
    "            km = KMeans(n_clusters=self.n_known)\n",
    "            km_labels = km.fit_predict(features)\n",
    "            final_labels = km_labels\n",
    "        \n",
    "        return final_labels\n",
    "    \n",
    "    def _estimate_cluster_num(self, features):\n",
    "        scores = []\n",
    "        for n in range(self.n_known, self.n_known+5):\n",
    "            km = KMeans(n_clusters=n)\n",
    "            labels = km.fit_predict(features)\n",
    "            scores.append(silhouette_score(features, labels))\n",
    "        return self.n_known + np.argmax(scores)\n",
    "    \n",
    "    def _is_close_to_any_known(self, feature, centers, threshold=0.5):\n",
    "        distances = [np.linalg.norm(feature - center) for center in centers]\n",
    "        return any(d < threshold for d in distances)\n",
    "\n",
    "# 训练流程（含可视化）\n",
    "def train_with_visualization(model, dataloader, epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    loss_fn = EnhancedGCDLoss().to(device)\n",
    "    \n",
    "    history = {'loss': [], 'ari': [], 'nmi': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for imgs, labels, mask in dataloader:\n",
    "            imgs, labels, mask = imgs.to(device), labels.to(device), mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            features = model(imgs)\n",
    "            loss = loss_fn(features, labels, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        # 评估与可视化\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            ari, nmi = evaluate_and_visualize(model, dataloader, device, epoch+1)\n",
    "            history['ari'].append(ari)\n",
    "            history['nmi'].append(nmi)\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, ARI={ari:.4f}, NMI={nmi:.4f}\")\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.title('Training Loss')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.plot(history['ari'])\n",
    "    plt.title('Adjusted Rand Index')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.plot(history['nmi'])\n",
    "    plt.title('Normalized Mutual Info')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_and_visualize(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls, _ in dataloader:\n",
    "            feats = model(imgs.to(device)).cpu().numpy()\n",
    "            features.append(feats)\n",
    "            labels.append(lbls.numpy())\n",
    "    \n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    clusterer = EnhancedClusterer(n_known=num_classes)\n",
    "    pred_labels = clusterer.fit_predict(features, labels)\n",
    "    \n",
    "    # 计算指标\n",
    "    known_mask = labels != -1\n",
    "    ari = adjusted_rand_score(labels[known_mask], pred_labels[known_mask]) if np.sum(known_mask) > 0 else float('nan')\n",
    "    nmi = normalized_mutual_info_score(labels[known_mask], pred_labels[known_mask]) if np.sum(known_mask) > 0 else float('nan')\n",
    "    \n",
    "    # 可视化特征分布\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_feats = tsne.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(tsne_feats[:,0], tsne_feats[:,1], c=labels, cmap='tab20', alpha=0.6)\n",
    "    plt.title(f'True Labels (Epoch {epoch})')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.scatter(tsne_feats[:,0], tsne_feats[:,1], c=pred_labels, cmap='tab20', alpha=0.6)\n",
    "    plt.title(f'Predicted Clusters (Epoch {epoch})')\n",
    "    plt.show()\n",
    "    \n",
    "    # 绘制混淆矩阵\n",
    "    if np.sum(known_mask) > 0:\n",
    "        cm = confusion_matrix(labels[known_mask], pred_labels[known_mask])\n",
    "        row_ind, col_ind = linear_sum_assignment(-cm)\n",
    "        cm = cm[row_ind][:, col_ind]\n",
    "        \n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted Clusters')\n",
    "        plt.ylabel('True Classes')\n",
    "        plt.title(f'Confusion Matrix (Epoch {epoch})')\n",
    "        plt.show()\n",
    "    \n",
    "    return ari, nmi\n",
    "\n",
    "# 初始化配置\n",
    "data_root = 'E:\\data1\\PlantDisease'\n",
    "num_classes = 38  # 修改为38类\n",
    "\n",
    "# 创建数据集和加载器\n",
    "dataset = SemiSupervisedPlantDoc(data_root, labeled_ratio=0.1)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# 初始化模型\n",
    "model = EnhancedDINOv2(\"E:\\models\\\\facebook_Dinov2\")\n",
    "\n",
    "# 开始训练\n",
    "train_with_visualization(model, loader, epochs=50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 928/928 [07:05<00:00,  2.18it/s]\n",
      "Epoch 2/100: 100%|██████████| 928/928 [06:56<00:00,  2.23it/s]\n",
      "Epoch 3/100: 100%|██████████| 928/928 [06:56<00:00,  2.23it/s]\n",
      "Epoch 4/100: 100%|██████████| 928/928 [06:53<00:00,  2.24it/s]\n",
      "Epoch 5/100: 100%|██████████| 928/928 [06:56<00:00,  2.23it/s]\n",
      "d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] 系统找不到指定的文件。\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\subprocess.py\", line 493, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\subprocess.py\", line 858, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\subprocess.py\", line 1327, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "d:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # 添加进度条库\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 自定义数据集类\n",
    "class PlantDocDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_path, transform=None, train=True, train_ratio=0.8, random_seed=42):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # 解析txt文件\n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('=')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            img_rel_path, label_str, _ = parts[0], parts[1], parts[2]\n",
    "            img_full_path = os.path.join(root_dir, 'images', img_rel_path.replace('/', os.path.sep))\n",
    "            if not os.path.exists(img_full_path):\n",
    "                continue\n",
    "            label = int(label_str)\n",
    "            self.samples.append((img_full_path, label))\n",
    "\n",
    "        # 随机分割数据集\n",
    "        num_samples = len(self.samples)\n",
    "        indices = list(range(num_samples))\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(train_ratio * num_samples)\n",
    "        self.indices = indices[:split_idx] if train else indices[split_idx:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        img_path, label = self.samples[actual_idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 初始化配置\n",
    "root_dir = r'E:/data1/plantdoc'\n",
    "txt_path = r'E:/data1/plantdoc/trainval.txt'\n",
    "\n",
    "# 已知种类数\n",
    "num_classes = 89\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = PlantDocDataset(\n",
    "    root_dir=root_dir,\n",
    "    txt_path=txt_path,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "test_dataset = PlantDocDataset(\n",
    "    root_dir=root_dir,\n",
    "    txt_path=txt_path,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 模型定义\n",
    "def load_efficientnet_b4(num_classes):\n",
    "    model = models.efficientnet_b4(weights='IMAGENET1K_V1')\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "def load_convnext_tiny(num_classes):\n",
    "    model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "    num_ftrs = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.efficientnet = load_efficientnet_b4(num_classes)\n",
    "        self.convnext = load_convnext_tiny(num_classes)\n",
    "        self.fc = nn.Linear(2*num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        eff_out = self.efficientnet(x)\n",
    "        conv_out = self.convnext(x)\n",
    "        return self.fc(torch.cat([eff_out, conv_out], dim=1))\n",
    "\n",
    "# 知识蒸馏损失\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, T=3, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.alpha = alpha\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_out, teacher_out, labels):\n",
    "        soft_loss = self.kl_loss(\n",
    "            nn.functional.log_softmax(student_out/self.T, dim=1),\n",
    "            nn.functional.softmax(teacher_out/self.T, dim=1)\n",
    "        ) * (self.alpha * self.T**2)\n",
    "        hard_loss = self.ce_loss(student_out, labels) * (1. - self.alpha)\n",
    "        return soft_loss + hard_loss\n",
    "\n",
    "# 增强聚类模块\n",
    "class EnhancedClusterer:\n",
    "    def __init__(self, n_known):\n",
    "        self.n_known = n_known\n",
    "        \n",
    "    def fit_predict(self, features, labels):\n",
    "        known_features = features[labels != -1]\n",
    "        known_labels = labels[labels != -1]\n",
    "        \n",
    "        # 计算已知类别中心\n",
    "        unique_known_labels = np.unique(known_labels)\n",
    "        if len(unique_known_labels) == 0:\n",
    "            return np.full_like(labels, -1), np.array([])\n",
    "        \n",
    "        known_centers = {c: known_features[known_labels == c].mean(0) for c in unique_known_labels}\n",
    "        \n",
    "        # 使用DBSCAN发现新类别\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        db_labels = dbscan.fit_predict(features)\n",
    "        \n",
    "        # 合并结果\n",
    "        final_labels = np.full_like(db_labels, -1)\n",
    "        new_label = max(unique_known_labels) + 1 if unique_known_labels.size > 0 else 0\n",
    "        new_labels = []\n",
    "        \n",
    "        for lbl in set(db_labels):\n",
    "            if lbl == -1: continue\n",
    "            cluster_idx = np.where(db_labels == lbl)[0]\n",
    "            cluster_feats = features[cluster_idx]\n",
    "            \n",
    "            if len(cluster_feats) < 5: continue\n",
    "            \n",
    "            # 寻找最近已知类别\n",
    "            closest = min(known_centers.keys(), \n",
    "                         key=lambda k: np.linalg.norm(cluster_feats.mean(0)-known_centers[k]))\n",
    "            \n",
    "            if closest is None or not self._is_close(cluster_feats.mean(0), \n",
    "                                                   list(known_centers.values()), 0.5):\n",
    "                final_labels[cluster_idx] = new_label\n",
    "                new_labels.append(new_label)\n",
    "                new_label += 1\n",
    "            else:\n",
    "                final_labels[cluster_idx] = closest\n",
    "                \n",
    "        # 确保最少聚类数\n",
    "        if len(np.unique(final_labels)) < self.n_known:\n",
    "            final_labels = KMeans(n_clusters=self.n_known).fit_predict(features)\n",
    "            unique_final_labels = np.unique(final_labels)\n",
    "            new_labels = unique_final_labels[unique_final_labels >= self.n_known]\n",
    "        \n",
    "        return final_labels, np.array(new_labels)\n",
    "    \n",
    "    def _is_close(self, feature, centers, threshold):\n",
    "        return any(np.linalg.norm(feature - c) < threshold for c in centers)\n",
    "\n",
    "# 训练流程\n",
    "def train_with_visualization(model, dataloader, epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    loss_fn = DistillationLoss().to(device)\n",
    "    \n",
    "    history = {'loss': [], 'ari': [], 'nmi': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # 添加进度条\n",
    "        for imgs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        # 每5轮评估\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            ari, nmi, num_predicted_clusters, num_true_classes = evaluate(model, dataloader, device)\n",
    "            history['ari'].append(ari)\n",
    "            history['nmi'].append(nmi)\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, ARI={ari:.4f}, NMI={nmi:.4f}, \"\n",
    "                  f\"Predicted Clusters={num_predicted_clusters}, True Classes={num_true_classes}\")\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(history['loss'], label='Loss')\n",
    "    plt.subplot(132)\n",
    "    plt.plot(history['ari'], label='ARI')\n",
    "    plt.subplot(133)\n",
    "    plt.plot(history['nmi'], label='NMI')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in dataloader:\n",
    "            feats = model(imgs.to(device)).cpu().numpy()\n",
    "            features.append(feats)\n",
    "            labels.append(lbls.numpy())\n",
    "    \n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    clusterer = EnhancedClusterer(n_known=num_classes)\n",
    "    pred_labels, new_labels = clusterer.fit_predict(features, labels)\n",
    "    \n",
    "    # 计算指标\n",
    "    ari = adjusted_rand_score(labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(labels, pred_labels)\n",
    "    \n",
    "    # 计算聚类结果的数量和真实类别的数量\n",
    "    num_predicted_clusters = len(np.unique(pred_labels))\n",
    "    num_true_classes = len(np.unique(labels))\n",
    "    \n",
    "    # 可视化\n",
    "    visualize_features(features, labels, pred_labels, new_labels)\n",
    "    return ari, nmi, num_predicted_clusters, num_true_classes\n",
    "\n",
    "def visualize_features(features, true_labels, pred_labels, new_labels):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embed = tsne.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(embed[:,0], embed[:,1], c=true_labels, cmap='tab20', alpha=0.6)\n",
    "    plt.title('True Labels')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    # 使用不同的颜色或标记来区分已知类别和新发现的类别\n",
    "    unique_pred_labels = np.unique(pred_labels)\n",
    "    for lbl in unique_pred_labels:\n",
    "        if lbl in new_labels:\n",
    "            plt.scatter(embed[pred_labels == lbl, 0], embed[pred_labels == lbl, 1], \n",
    "                        label=f'New {lbl}', alpha=0.6, marker='x')\n",
    "        else:\n",
    "            plt.scatter(embed[pred_labels == lbl, 0], embed[pred_labels == lbl, 1], \n",
    "                        label=f'Known {lbl}', alpha=0.6, marker='o')\n",
    "    plt.title('Predicted Clusters')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 初始化模型\n",
    "model = FusionNet(num_classes=num_classes)\n",
    "\n",
    "# 开始训练\n",
    "train_with_visualization(model, train_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   5%|▍         | 46/928 [00:20<06:38,  2.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 280\u001b[0m\n\u001b[0;32m    277\u001b[0m model \u001b[38;5;241m=\u001b[39m FusionNet(num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m \u001b[43mtrain_with_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 212\u001b[0m, in \u001b[0;36mtrain_with_visualization\u001b[1;34m(model, dataloader, epochs)\u001b[0m\n\u001b[0;32m    210\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, outputs, labels)\n\u001b[0;32m    211\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 212\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    216\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\adamw.py:227\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         state_steps,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 227\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\adamw.py:767\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    765\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 767\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\adamw.py:580\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    578\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 580\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    582\u001b[0m     ]\n\u001b[0;32m    583\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    585\u001b[0m     ]\n\u001b[0;32m    587\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\adamw.py:581\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    578\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 581\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    582\u001b[0m     ]\n\u001b[0;32m    583\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[0;32m    585\u001b[0m     ]\n\u001b[0;32m    587\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[1;32md:\\Users\\songyanghui\\anaconda3\\envs\\mvpdr\\lib\\site-packages\\torch\\optim\\optimizer.py:104\u001b[0m, in \u001b[0;36m_get_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # 添加进度条库\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 自定义数据集类\n",
    "class PlantDocDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_path, transform=None, train=True, train_ratio=0.8, random_seed=42):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # 解析txt文件\n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('=')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            img_rel_path, label_str, _ = parts[0], parts[1], parts[2]\n",
    "            img_full_path = os.path.join(root_dir, 'images', img_rel_path.replace('/', os.path.sep))\n",
    "            if not os.path.exists(img_full_path):\n",
    "                continue\n",
    "            label = int(label_str)\n",
    "            self.samples.append((img_full_path, label))\n",
    "\n",
    "        # 随机分割数据集\n",
    "        num_samples = len(self.samples)\n",
    "        indices = list(range(num_samples))\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(train_ratio * num_samples)\n",
    "        self.indices = indices[:split_idx] if train else indices[split_idx:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        img_path, label = self.samples[actual_idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 初始化配置\n",
    "root_dir = r'E:/data1/plantdoc'\n",
    "txt_path = r'E:/data1/plantdoc/trainval.txt'\n",
    "\n",
    "# 已知种类数\n",
    "num_classes = 89\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = PlantDocDataset(\n",
    "    root_dir=root_dir,\n",
    "    txt_path=txt_path,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "test_dataset = PlantDocDataset(\n",
    "    root_dir=root_dir,\n",
    "    txt_path=txt_path,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 模型定义\n",
    "def load_efficientnet_b4(num_classes):\n",
    "    model = models.efficientnet_b4(weights='IMAGENET1K_V1')\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "def load_convnext_tiny(num_classes):\n",
    "    model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "    num_ftrs = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.efficientnet = load_efficientnet_b4(num_classes)\n",
    "        self.convnext = load_convnext_tiny(num_classes)\n",
    "        self.fc = nn.Linear(2*num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        eff_out = self.efficientnet(x)\n",
    "        conv_out = self.convnext(x)\n",
    "        return self.fc(torch.cat([eff_out, conv_out], dim=1))\n",
    "\n",
    "# 知识蒸馏损失\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, T=3, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.alpha = alpha\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_out, teacher_out, labels):\n",
    "        soft_loss = self.kl_loss(\n",
    "            nn.functional.log_softmax(student_out/self.T, dim=1),\n",
    "            nn.functional.softmax(teacher_out/self.T, dim=1)\n",
    "        ) * (self.alpha * self.T**2)\n",
    "        hard_loss = self.ce_loss(student_out, labels) * (1. - self.alpha)\n",
    "        return soft_loss + hard_loss\n",
    "\n",
    "# 增强聚类模块\n",
    "class EnhancedClusterer:\n",
    "    def __init__(self, n_known):\n",
    "        self.n_known = n_known\n",
    "        \n",
    "    def fit_predict(self, features, labels):\n",
    "        known_features = features[labels != -1]\n",
    "        known_labels = labels[labels != -1]\n",
    "        \n",
    "        # 计算已知类别中心\n",
    "        unique_known_labels = np.unique(known_labels)\n",
    "        if len(unique_known_labels) == 0:\n",
    "            return np.full_like(labels, -1)\n",
    "        \n",
    "        known_centers = {c: known_features[known_labels == c].mean(0) for c in unique_known_labels}\n",
    "        \n",
    "        # 使用DBSCAN发现新类别\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        db_labels = dbscan.fit_predict(features)\n",
    "        \n",
    "        # 合并结果\n",
    "        final_labels = np.full_like(db_labels, -1)\n",
    "        new_label = max(unique_known_labels) + 1 if unique_known_labels.size > 0 else 0\n",
    "        \n",
    "        for lbl in set(db_labels):\n",
    "            if lbl == -1: continue\n",
    "            cluster_idx = np.where(db_labels == lbl)[0]\n",
    "            cluster_feats = features[cluster_idx]\n",
    "            \n",
    "            if len(cluster_feats) < 5: continue\n",
    "            \n",
    "            # 寻找最近已知类别\n",
    "            closest = min(known_centers.keys(), \n",
    "                         key=lambda k: np.linalg.norm(cluster_feats.mean(0)-known_centers[k]))\n",
    "            \n",
    "            if closest is None or not self._is_close(cluster_feats.mean(0), \n",
    "                                                   list(known_centers.values()), 0.5):\n",
    "                final_labels[cluster_idx] = new_label\n",
    "                new_label += 1\n",
    "            else:\n",
    "                final_labels[cluster_idx] = closest\n",
    "                \n",
    "        # 确保最少聚类数\n",
    "        if len(np.unique(final_labels)) < self.n_known:\n",
    "            final_labels = KMeans(n_clusters=self.n_known).fit_predict(features)\n",
    "            \n",
    "        return final_labels\n",
    "    \n",
    "    def _is_close(self, feature, centers, threshold):\n",
    "        return any(np.linalg.norm(feature - c) < threshold for c in centers)\n",
    "\n",
    "# 训练流程\n",
    "def train_with_visualization(model, dataloader, epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    loss_fn = DistillationLoss().to(device)\n",
    "    \n",
    "    history = {'loss': [], 'ari': [], 'nmi': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # 添加进度条\n",
    "        for imgs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        # 每5轮评估\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            ari, nmi = evaluate(model, dataloader, device)\n",
    "            history['ari'].append(ari)\n",
    "            history['nmi'].append(nmi)\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, ARI={ari:.4f}, NMI={nmi:.4f}\")\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(history['loss'], label='Loss')\n",
    "    plt.subplot(132)\n",
    "    plt.plot(history['ari'], label='ARI')\n",
    "    plt.subplot(133)\n",
    "    plt.plot(history['nmi'], label='NMI')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in dataloader:\n",
    "            feats = model(imgs.to(device)).cpu().numpy()\n",
    "            features.append(feats)\n",
    "            labels.append(lbls.numpy())\n",
    "    \n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    clusterer = EnhancedClusterer(n_known=num_classes)\n",
    "    pred_labels = clusterer.fit_predict(features, labels)\n",
    "    \n",
    "    # 计算指标\n",
    "    ari = adjusted_rand_score(labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(labels, pred_labels)\n",
    "    \n",
    "    # 可视化\n",
    "    visualize_features(features, labels, pred_labels)\n",
    "    return ari, nmi\n",
    "\n",
    "def visualize_features(features, true_labels, pred_labels):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embed = tsne.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(embed[:,0], embed[:,1], c=true_labels, cmap='tab20', alpha=0.6)\n",
    "    plt.title('True Labels')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.scatter(embed[:,0], embed[:,1], c=pred_labels, cmap='tab20', alpha=0.6)\n",
    "    plt.title('Predicted Clusters')\n",
    "    plt.show()\n",
    "\n",
    "# 初始化模型\n",
    "model = FusionNet(num_classes=num_classes)\n",
    "\n",
    "# 开始训练\n",
    "train_with_visualization(model, train_loader, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/1855 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # 添加进度条库\n",
    "import logging\n",
    "import psutil  # 用于监控内存使用\n",
    "\n",
    "# 设置环境变量以限制 OpenBLAS 线程数\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"24\"  # 充分利用所有核心\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"24\"\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 配置日志记录\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 自定义数据集类\n",
    "class PlantDocDataset(Dataset):\n",
    "    def __init__(self, root_dir, txt_path, transform=None, train=True, train_ratio=0.8, random_seed=42):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # 解析txt文件\n",
    "        with open(txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('=')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            img_rel_path, label_str, _ = parts[0], parts[1], parts[2]\n",
    "            img_full_path = os.path.join(root_dir, 'images', img_rel_path.replace('/', os.path.sep))\n",
    "            if not os.path.exists(img_full_path):\n",
    "                continue\n",
    "            label = int(label_str)\n",
    "            self.samples.append((img_full_path, label))\n",
    "\n",
    "        # 随机分割数据集\n",
    "        num_samples = len(self.samples)\n",
    "        indices = list(range(num_samples))\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(train_ratio * num_samples)\n",
    "        self.indices = indices[:split_idx] if train else indices[split_idx:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        img_path, label = self.samples[actual_idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 初始化配置\n",
    "root_dir = r'E:/data1/plantdoc'\n",
    "txt_path = r'E:/data1/plantdoc/trainval.txt'\n",
    "\n",
    "# 已知种类数\n",
    "num_classes = 89\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = PlantDocDataset(\n",
    "    root_dir=root_dir,\n",
    "    txt_path=txt_path,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "test_dataset = PlantDocDataset(\n",
    "    root_dir=root_dir,\n",
    "    txt_path=txt_path,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=24)  # 增加 num_workers\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=24)    # 增加 num_workers\n",
    "\n",
    "# 简化模型\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.efficientnet_b0(weights='IMAGENET1K_V1')  # 使用更小的 EfficientNet\n",
    "        num_ftrs = self.model.classifier[1].in_features\n",
    "        self.model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 知识蒸馏损失\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, T=3, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.alpha = alpha\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_out, teacher_out, labels):\n",
    "        soft_loss = self.kl_loss(\n",
    "            nn.functional.log_softmax(student_out/self.T, dim=1),\n",
    "            nn.functional.softmax(teacher_out/self.T, dim=1)\n",
    "        ) * (self.alpha * self.T**2)\n",
    "        hard_loss = self.ce_loss(student_out, labels) * (1. - self.alpha)\n",
    "        return soft_loss + hard_loss\n",
    "\n",
    "# 增强聚类模块\n",
    "class EnhancedClusterer:\n",
    "    def __init__(self, n_known):\n",
    "        self.n_known = n_known\n",
    "        \n",
    "    def fit_predict(self, features, labels):\n",
    "        known_features = features[labels != -1]\n",
    "        known_labels = labels[labels != -1]\n",
    "        \n",
    "        # 计算已知类别中心\n",
    "        unique_known_labels = np.unique(known_labels)\n",
    "        if len(unique_known_labels) == 0:\n",
    "            return np.full_like(labels, -1), np.array([])\n",
    "        \n",
    "        known_centers = {c: known_features[known_labels == c].mean(0) for c in unique_known_labels}\n",
    "        \n",
    "        # 使用DBSCAN发现新类别\n",
    "        dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "        db_labels = dbscan.fit_predict(features)\n",
    "        \n",
    "        # 合并结果\n",
    "        final_labels = np.full_like(db_labels, -1)\n",
    "        new_label = max(unique_known_labels) + 1 if unique_known_labels.size > 0 else 0\n",
    "        new_labels = []\n",
    "        \n",
    "        for lbl in set(db_labels):\n",
    "            if lbl == -1: continue\n",
    "            cluster_idx = np.where(db_labels == lbl)[0]\n",
    "            cluster_feats = features[cluster_idx]\n",
    "            \n",
    "            if len(cluster_feats) < 5: continue\n",
    "            \n",
    "            # 寻找最近已知类别\n",
    "            closest = min(known_centers.keys(), \n",
    "                         key=lambda k: np.linalg.norm(cluster_feats.mean(0)-known_centers[k]))\n",
    "            \n",
    "            if closest is None or not self._is_close(cluster_feats.mean(0), \n",
    "                                                   list(known_centers.values()), 0.5):\n",
    "                final_labels[cluster_idx] = new_label\n",
    "                new_labels.append(new_label)\n",
    "                new_label += 1\n",
    "            else:\n",
    "                final_labels[cluster_idx] = closest\n",
    "                \n",
    "        # 确保最少聚类数\n",
    "        if len(np.unique(final_labels)) < self.n_known:\n",
    "            final_labels = KMeans(n_clusters=self.n_known).fit_predict(features)\n",
    "            unique_final_labels = np.unique(final_labels)\n",
    "            new_labels = unique_final_labels[unique_final_labels >= self.n_known]\n",
    "        \n",
    "        return final_labels, np.array(new_labels)\n",
    "    \n",
    "    def _is_close(self, feature, centers, threshold):\n",
    "        return any(np.linalg.norm(feature - c) < threshold for c in centers)\n",
    "\n",
    "# 训练流程\n",
    "def train_with_visualization(model, dataloader, epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    loss_fn = DistillationLoss().to(device)\n",
    "    \n",
    "    history = {'loss': [], 'ari': [], 'nmi': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # 添加进度条\n",
    "        for imgs, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        \n",
    "        # 每5轮评估\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            ari, nmi, num_predicted_clusters, num_true_classes = evaluate(model, dataloader, device)\n",
    "            history['ari'].append(ari)\n",
    "            history['nmi'].append(nmi)\n",
    "            logger.info(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, ARI={ari:.4f}, NMI={nmi:.4f}, \"\n",
    "                        f\"Predicted Clusters={num_predicted_clusters}, True Classes={num_true_classes}\")\n",
    "        \n",
    "        # 监控内存使用\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        logger.info(f\"Memory Usage: {memory_usage}%\")\n",
    "    \n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(history['loss'], label='Loss')\n",
    "    plt.subplot(132)\n",
    "    plt.plot(history['ari'], label='ARI')\n",
    "    plt.subplot(133)\n",
    "    plt.plot(history['nmi'], label='NMI')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in dataloader:\n",
    "            feats = model(imgs.to(device)).cpu().numpy()\n",
    "            features.append(feats)\n",
    "            labels.append(lbls.numpy())\n",
    "    \n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    clusterer = EnhancedClusterer(n_known=num_classes)\n",
    "    pred_labels, new_labels = clusterer.fit_predict(features, labels)\n",
    "    \n",
    "    # 计算指标\n",
    "    ari = adjusted_rand_score(labels, pred_labels)\n",
    "    nmi = normalized_mutual_info_score(labels, pred_labels)\n",
    "    \n",
    "    # 计算聚类结果的数量和真实类别的数量\n",
    "    num_predicted_clusters = len(np.unique(pred_labels))\n",
    "    num_true_classes = len(np.unique(labels))\n",
    "    \n",
    "    # 可视化\n",
    "    visualize_features(features, labels, pred_labels, new_labels)\n",
    "    return ari, nmi, num_predicted_clusters, num_true_classes\n",
    "\n",
    "def visualize_features(features, true_labels, pred_labels, new_labels):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embed = tsne.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(211)\n",
    "    plt.scatter(embed[:,0], embed[:,1], c=true_labels, cmap='tab20', alpha=0.6)\n",
    "    plt.title('True Labels')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    # 使用不同的颜色或标记来区分已知类别和新发现的类别\n",
    "    unique_pred_labels = np.unique(pred_labels)\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for lbl in unique_pred_labels:\n",
    "        if lbl in new_labels:\n",
    "            scatter = plt.scatter(embed[pred_labels == lbl, 0], embed[pred_labels == lbl, 1], \n",
    "                                  label=f'New {lbl}', alpha=0.6, marker='x')\n",
    "        else:\n",
    "            scatter = plt.scatter(embed[pred_labels == lbl, 0], embed[pred_labels == lbl, 1], \n",
    "                                  label=f'Known {lbl}', alpha=0.6, marker='o')\n",
    "        handles.append(scatter)\n",
    "        labels.append(scatter.get_label())\n",
    "    \n",
    "    plt.title('Predicted Clusters')\n",
    "    plt.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=5)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# 初始化模型\n",
    "model = SimpleNet(num_classes=num_classes)\n",
    "\n",
    "# 开始训练\n",
    "try:\n",
    "    train_with_visualization(model, train_loader, epochs=100)\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvpdr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
